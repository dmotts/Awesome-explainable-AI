# Feature Attribution/Importance

## Surveys 

[Gradient based Feature Attribution in Explainable AI: A
Technical Review](https://arxiv.org/pdf/2403.10415.pdf), Arxiv Preprint

## Papers

[An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records](https://arxiv.org/abs/2406.08958), EMNLP 2024, [Blog](https://joakimedin.substack.com/p/an-unsupervised-approach-to-achieve)

[LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack](https://ojs.aaai.org/index.php/AAAI/article/view/29950/31660), AAAI 2024

[Integrated Decision Gradients: Compute Your Attributions Where the Model Makes Its Decision](https://ojs.aaai.org/index.php/AAAI/article/view/28336/28660), AAAI 2024

[Using stratified sampling to improve LIME Image explanations](https://ojs.aaai.org/index.php/AAAI/article/view/29397/30639), AAAI 2024

[Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention](https://ojs.aaai.org/index.php/AAAI/article/view/28077/28160), AAAI 2024

[Empowering CAM-Based Methods with Capability to Generate Fine-Grained and High-Faithfulness Explanations](https://arxiv.org/pdf/2303.09171), AAAI 2024

[Beyond TreeSHAP: Efficient Computation of anyorder Shapley Interactions for Tree Ensembles](https://ojs.aaai.org/index.php/AAAI/article/view/29352/30552), AAAI 2024

[SHAP@k: Efficient and Probably Approximately Correct (PAC) Identification of Top-K Features](https://ojs.aaai.org/index.php/AAAI/article/view/29205), AAAI 2024

[Approximating the Shapley Value without Marginal Contributions](https://ojs.aaai.org/index.php/AAAI/article/view/29225/30311), AAAI 2024

[GLIME: General, Stable and Local LIME Explanation](https://proceedings.neurips.cc/paper_files/paper/2023/file/71ed042903ed67c7f6355e5dd0539eec-Paper-Conference.pdf), NIPS 2024

[Deeply Explain CNN via Hierarchical Decomposition](https://arxiv.org/pdf/2201.09205.pdf), IJCV 2023

[Negative Flux Aggregation to Estimate Feature Attributions](https://www.ijcai.org/proceedings/2023/0050.pdf), IJCAI 2023, [code](https://github.com/xinli0928/NeFLAG)

[On Minimizing the Impact of Dataset Shifts on Actionable Explanations](https://arxiv.org/pdf/2306.06716.pdf), UAI 2023

[Counterfactual-based Saliency Map: Towards Visual Contrastive Explanations for Neural Networks](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Counterfactual-based_Saliency_Map_Towards_Visual_Contrastive_Explanations_for_Neural_Networks_ICCV_2023_paper.pdf), CVPR 2023

[A Practical Upper Bound for the Worst-Case Attribution Deviations](https://arxiv.org/pdf/2303.00340.pdf), CVPR 2023

[IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_IDGI_A_Framework_To_Eliminate_Explanation_Noise_From_Integrated_Gradients_CVPR_2023_paper.pdf), CVPR 2023

[Explaining Image Classifiers with Multiscale Directional Image Representation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kolek_Explaining_Image_Classifiers_With_Multiscale_Directional_Image_Representation_CVPR_2023_paper.pdf), CVPR 2023

[SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries](https://openaccess.thecvf.com/content/CVPR2023/papers/Humayun_SplineCam_Exact_Visualization_and_Characterization_of_Deep_Network_Geometry_and_CVPR_2023_paper.pdf), CVPR 2023

[Extending class activation mapping using Gaussian receptive field](https://www.sciencedirect.com/science/article/abs/pii/S1077314223000437), CVIU Journal 2023

[TSGB: Target-selective gradient backprop for probing CNN visual saliency](https://arxiv.org/pdf/2110.05182), TIP 2022

[Transferable Adversarial Attack Based on Integrated Gradients](https://openreview.net/pdf?id=DesNW4-5ai9), ICLR 2022

[OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_OrphicX_A_Causality-Inspired_Latent_Variable_Model_for_Interpreting_Graph_Neural_CVPR_2022_paper.pdf), CVPR 2022

[Consistent Explanations by Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2022/papers/Pillai_Consistent_Explanations_by_Contrastive_Learning_CVPR_2022_paper.pdf), CVPR 2022

[VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Aflalo_VL-InterpreT_An_Interactive_Visualization_Tool_for_Interpreting_Vision-Language_Transformers_CVPR_2022_paper.pdf), CVPR 2022

[REX: Reasoning-aware and Grounded Explanation](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_REX_Reasoning-Aware_and_Grounded_Explanation_CVPR_2022_paper.pdf), CVPR 2022

[FAM: Visual Explanations for the Feature Representations from Deep Convolutional Networks](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_FAM_Visual_Explanations_for_the_Feature_Representations_From_Deep_Convolutional_CVPR_2022_paper.pdf), CVPR 2022

[NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2022/papers/Sammani_NLX-GPT_A_Model_for_Natural_Language_Explanations_in_Vision_and_CVPR_2022_paper.pdf), CVPR 2022

[Do Explanations Explain? Model Knows Best](https://openaccess.thecvf.com/content/CVPR2022/papers/Khakzar_Do_Explanations_Explain_Model_Knows_Best_CVPR_2022_paper.pdf), CVPR 2022

[On Computing Probabilistic Explanations for Decision Trees](https://openreview.net/pdf?id=zD65Zdh6ZhI), NeurIPS 2022

[Exploiting the Relationship Between Kendall’s Rank Correlation and Cosine Similarity for Attribution Protection](https://arxiv.org/pdf/2205.07279.pdf), NeurIPS 2022

[Linear TreeShap](https://openreview.net/pdf?id=OzbkiUo24g), NeurIPS 2022

[CS-SHAPLEY: Class-wise Shapley Values for Data Valuation in Classification](https://openreview.net/pdf?id=KTOcrOR5mQ9), NeurIPS 2022

[Consistent Sufficient Explanations and Minimal Local Rules for explaining any classifier or regressor](https://openreview.net/pdf?id=kHNKDNLVp1E), NeurIPS 2022

[New Definitions and Evaluations for Saliency Methods: Staying Intrinsic, Complete and Sound](https://proceedings.neurips.cc/paper_files/paper/2022/hash/d6383e7643415842b48a5077a1b09c98-Abstract-Conference.html), NeurIPS 2022


[Accurate Shapley Values for explaining tree-based models](https://proceedings.mlr.press/v151/amoukou22a/amoukou22a.pdf), AISTATS 2022

[Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations](https://openreview.net/pdf?id=rTvH1_SRyXs), NeurIPS 2022

[What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods](https://openreview.net/pdf?id=59pMU2xFxG), NeurIPS 2022

[Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF](https://openreview.net/pdf?id=FhuM-kk8Pbk), NeurIPS 2022

[What You See is What You Classify: Black Box Attributions](https://openreview.net/pdf?id=I-ggHgon-Az), NeurIPS 2022

[Explaining Preferences with Shapley Values](https://openreview.net/pdf?id=-me36V0os8P), NeurIPS 2022

[Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability](https://openreview.net/pdf?id=Nf_XI3uVGaZ), NeurIPS 2022

[Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability](https://openreview.net/pdf?id=ddPXQt-gM--), NeurIPS 2022

[Is this the Right Neighborhood? Accurate and Query Efficient Model Agnostic Explanations](https://openreview.net/pdf?id=lJHkZbX6Ic1), NeurIPS 2022

[Bayesian subset selection and variable importance for interpretable prediction and classification](https://jmlr.org/papers/volume23/21-0403/21-0403.pdf), NeurIPS 2022

[Robust Models Are More Interpretable Because Attributions Look Normal](https://arxiv.org/abs/2103.11257), ICML 2022

[Accelerating Shapley Explanation via Contributive Cooperator Selection](https://arxiv.org/pdf/2206.08529.pdf), ICML 2022

[Framework for Evaluating Faithfulness of Local Explanations](https://arxiv.org/pdf/2202.00734.pdf), ICML 2022

[XAI for Transformers: Better Explanations through Conservative Propagation](https://arxiv.org/pdf/2202.07304.pdf), ICML 2022

[A Functional Information Perspective on Model Interpretation](https://arxiv.org/pdf/2206.05700.pdf), ICML 2022

[A Psychological Theory of Explainability](https://arxiv.org/pdf/2205.08452.pdf), ICML 2022

[A Consistent and Efficient Evaluation Strategy
for Attribution Methods](https://arxiv.org/pdf/2202.00449.pdf), ICML 2022

[A Rigorous Study of Integrated Gradients Method
and Extensions to Internal Neuron Attributions](https://arxiv.org/pdf/2202.11912v2.pdf), ICML 2022

[Interpretable Neural Networks with Frank-Wolfe:
Sparse Relevance Maps and Relevance Orderings](https://proceedings.mlr.press/v162/macdonald22a/macdonald22a.pdf), ICML 2022

[Rational Shapley Values](https://facctconference.org/static/pdfs_2022/facct22-89.pdf), FAccT 2022

[Human Interpretation of Saliency-based Explanation Over Text](https://facctconference.org/static/pdfs_2022/facct22-51.pdf), FAccT 2022

[Higher-Order Explanations of Graph Neural Networks via Relevant Walks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547794), TPAMI 2022

[Explaining Explanations: Axiomatic Feature Interactions for
Deep Networks](https://jmlr.csail.mit.edu/papers/volume22/20-1223/20-1223.pdf), JMLR 2022

[Counterfactual Interpolation Augmentation (CIA): A Unified Approach to Enhance Fairness and Explainability of DNN](https://dongxiaozhu.github.io/assets/files/IJCAI2022CIA.pdf), IJCAI 2022

[DERIVING EXPLAINABLE DISCRIMINATIVE ATTRIBUTES USING CONFUSION ABOUT COUNTERFACTUAL CLASS](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747693), ICASSP 2022

[FastSHAP: Real-Time Shapley Value Estimation](https://openreview.net/pdf?id=Zq2G_VTV53T), ICLR 2022

[Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations](https://arxiv.org/pdf/2112.09669.pdf), AAAI 2022

[Backdoor Attacks on the DNN Interpretation System](https://arxiv.org/pdf/2011.10698.pdf), AAAI 2022

[Feature Importance Explanations for Temporal Black-Box Models](https://arxiv.org/pdf/2102.11934.pdf), AAAI 2022

[Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?](https://www2.cs.sfu.ca/~hamarneh/ecopy/aaai2022.pdf), AAAI 2022

[Do Feature Attribution Methods Correctly Attribute Features?](https://arxiv.org/pdf/2104.14403.pdf), AAAI 2022

[Improving performance of deep learning models with axiomatic attribution priors and expected gradients.](https://www.nature.com/articles/s42256-021-00343-w), Nature Machine Intelligence 2021

[One Explanation is Not Enough: Structured Attention Graphs for Image Classification](https://openreview.net/pdf?id=k5Kbs9uPGP9), NeurIPS 2021

[On Locality of Local Explanation Models](https://arxiv.org/abs/2106.14648), NeurIPS 2021

[Shapley Residuals: Quantifying the limits of the Shapley value for explanations](https://par.nsf.gov/servlets/purl/10187138), NeurIPS 2021

[The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations](https://arxiv.org/abs/2106.00786), NeurIPS 2021

[Reliable Post hoc Explanations: Modeling Uncertainty in Explainability](https://arxiv.org/abs/2008.05030), NeurIPS 2021

[Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis](https://arxiv.org/abs/2111.04138), NeurIPS 2021

[Do Input Gradients Highlight Discriminative Features?](https://arxiv.org/abs/2102.12781), NeurIPS 2021

[The effectiveness of feature attribution methods and its correlation with automatic evaluation scores](https://arxiv.org/abs/2105.14944), NeurIPS 2021

[From global to local MDI variable importances for random forests and when they are Shapley values](https://arxiv.org/abs/2111.02218), NeurIPS 2021 

[Fast Axiomatic Attribution for Neural Networks](https://proceedings.neurips.cc/paper/2021/file/a284df1155ec3e67286080500df36a9a-Paper.pdf), NeurIPS 2021

[On Guaranteed Optimal Robust Explanations for NLP Models](https://arxiv.org/pdf/2105.03640.pdf), IJCAI 2021

[ Explaining deep neural network models with adversarial gradient integration](https://www.ijcai.org/proceedings/2021/0396.pdf), IJCAI 2021

[Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models](https://aclanthology.org/2021.acl-long.71.pdf), ACL 2021

[What does LIME really see in images?](https://arxiv.org/abs/2102.06307), ICML 2021

[Explanations for Monotonic Classifiers](https://arxiv.org/pdf/2106.00154.pdf), ICML 2021

[Explaining Time Series Predictions with Dynamic Masks](https://arxiv.org/pdf/2106.05303.pdf), ICML 2021

[On Explainability of Graph Neural Networks via Subgraph Explorations](http://proceedings.mlr.press/v139/yuan21c.html), ICML 2021

[Generative Causal Explanations for Graph Neural Networks](http://proceedings.mlr.press/v139/lin21d/lin21d.pdf), ICML 2021

[Explaining Explanations: Axiomatic Feature Interactions for Deep Networks](https://jmlr.csail.mit.edu/papers/volume22/20-1223/20-1223.pdf), ICML 2021

[How Interpretable and Trustworthy are GAMs?](https://arxiv.org/pdf/2006.06466.pdf), KDD 2021

[Leveraging Latent Features for Local Explanations](https://arxiv.org/abs/1905.12698), KDD 2021

[S-LIME: Stabilized-LIME for Model Explanation](https://arxiv.org/abs/2106.07875), KDD 2021

[An Experimental Study of Quantitative Evaluations on Saliency Methods](https://arxiv.org/pdf/2012.15616.pdf), KDD 2021

[TimeSHAP: Explaining Recurrent Models through Sequence Perturbations](https://arxiv.org/abs/2012.00073), KDD 2021

[Black-box Explanation of Object Detectors via Saliency Maps](https://openaccess.thecvf.com/content/CVPR2021/papers/Petsiuk_Black-Box_Explanation_of_Object_Detectors_via_Saliency_Maps_CVPR_2021_paper.pdf), CVPR 2021 [Interpreting Super-Resolution Networks with Local Attribution Maps](https://openaccess.thecvf.com/content/CVPR2021/papers/Gu_Interpreting_Super-Resolution_Networks_With_Local_Attribution_Maps_CVPR_2021_paper.pdf), CVPR 2021

[Transformer Interpretability Beyond Attention Visualization](https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf), CVPR 2021

[A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts](https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf), CVPR 2021

[Building Reliable Explanations of Unreliable Neural Networks: Locally Smoothing Perspective of Model Interpretation](https://openaccess.thecvf.com/content/CVPR2021/papers/Lim_Building_Reliable_Explanations_of_Unreliable_Neural_Networks_Locally_Smoothing_Perspective_CVPR_2021_paper.pdf), CVPR 2021

[Relevance-CAM: Your Model Already Knows Where to Look](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Relevance-CAM_Your_Model_Already_Knows_Where_To_Look_CVPR_2021_paper.pdf), CVPR 2021, [code](https://github.com/mongeoroo/Relevance-CAM)

[Guided integrated gradients: An adaptive path method for removing noise](https://openaccess.thecvf.com/content/CVPR2021/papers/Kapishnikov_Guided_Integrated_Gradients_An_Adaptive_Path_Method_for_Removing_Noise_CVPR_2021_paper.pdf), CVPR 2021

[An Analysis of LIME for Text Data](https://arxiv.org/pdf/2010.12487.pdf), AISTATS 2021

[Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression](https://arxiv.org/pdf/2012.01536.pdf), AISTATS 2021

[A Unified Taylor Framework for Revisiting Attribution Methods](https://arxiv.org/abs/2008.09695), AAAI 2021

[If You Like Shapley Then You’ll Love the Core](https://ojs.aaai.org/index.php/AAAI/article/view/16721/16528), AAAI 2021

[Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations](https://arxiv.org/pdf/2012.03434.pdf), AAAI 2021

[Explaining Convolutional Neural Networks through Attribution-Based Input Sampling and Block-Wise Feature Aggregation](https://arxiv.org/pdf/2010.00672.pdf), AAAI 2021

[Explainable Models with Consistent Interpretations](https://www.csee.umbc.edu/~hpirsiav/papers/gc_aaai21.pdf), AAAI 2021

[On the Tractability of SHAP Explanations](https://arxiv.org/abs/2009.08634), AAAI 2021

[Interpreting Multivariate Shapley Interactions in DNNs](https://arxiv.org/abs/2010.05045), AAAI 2021

[Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations](https://arxiv.org/pdf/2012.03434.pdf), AAAI 2021

[Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking](https://openreview.net/forum?id=WznmQa42ZAx), ICLR 2021

[Scaling Symbolic Methods using Gradients for Neural Model Explanation](https://openreview.net/forum?id=V5j-jdoDDP), ICLR 2021

[Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability](https://openreview.net/forum?id=dYeAHXnpWJ4), ICLR 2021

[Shapley explainability on the data manifold](https://openreview.net/forum?id=OPyWRrcjVQw), ICLR 2021

[ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping](https://arxiv.org/abs/2006.08287), NeurIPS 2020

[What went wrong and when? Instance-wise Feature Importance for Time-series Models](https://arxiv.org/abs/2003.02821), NeurIPS 2020

[How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods](https://proceedings.neurips.cc/paper/2020/file/2c29d89cc56cdb191c60db2f0bae796b-Paper.pdf), NeurIPS 2020 [code](https://github.com/nesl/Explainability-Study)

[Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability](https://papers.nips.cc/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf), NeurIPS 2020

[Parameterized Explainer for Graph Neural Network](https://proceedings.neurips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf), NeurIPS 2020

[PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks](https://proceedings.neurips.cc/paper/2020/file/8fb134f258b1f7865a6ab2d935a897c9-Paper.pdf), NeurIPS 2020

[Visualizing the Impact of Feature Attribution Baselines](https://distill.pub/2020/attribution-baselines/), Distill 2020

[There and Back Again: Revisiting Backpropagation Saliency Methods](https://openaccess.thecvf.com/content_CVPR_2020/papers/Rebuffi_There_and_Back_Again_Revisiting_Backpropagation_Saliency_Methods_CVPR_2020_paper.pdf), CVPR 2020

[Towards Visually Explaining Variational Autoencoders](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Towards_Visually_Explaining_Variational_Autoencoders_CVPR_2020_paper.pdf), CVPR 2020

Blur integrated gradient: [Attribution in Scale and Space](https://arxiv.org/pdf/2004.03383.pdf), CVPR 2020

[Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution](https://arxiv.org/pdf/2004.10484.pdf) arxiv preprint 2020

[GCN-LRP explanation: exploring latent attention of graph convolutional networks](https://ieeexplore.ieee.org/document/9207639), IJCNN 2020

[Visualizing Deep Networks by Optimizing with Integrated Gradients](https://aaai.org/Papers/AAAI/2020GB/AAAI-QiZ.4029.pdf), AAAI 2020

[Relative Attributing Propagation: Interpreting the Comparative Contributions of Individual Units in Deep Neural Networks](https://arxiv.org/pdf/1904.00605.pdf), AAAI 2020

[LS-Tree: Model Interpretation When the Data Are Linguistic](https://ojs.aaai.org/index.php/AAAI/article/view/5749/5605), AAAI 2020, [slides](https://www.jianbochen.me/files/slides_ls_tree.pdf)

[Investigating Saturation Effects in Integrated Gradients](https://arxiv.org/pdf/2010.12697.pdf), ICMLW on WHI 2020

[Robust and Stable Black Box Explanations](https://proceedings.icml.cc/static/paper_files/icml/2020/5945-Paper.pdf), ICML 2020

[Concise Explanations of Neural Networks using Adversarial Training](https://arxiv.org/pdf/1810.06583.pdf), ICML 2020

[TOWARDS HIERARCHICAL IMPORTANCE ATTRIBUTION: EXPLAINING COMPOSITIONAL SEMANTICS FOR NEURAL SEQUENCE MODELS](https://iclr.cc/virtual_2020/poster_BkxRRkSKwr.html), ICLR 2020

[Feature relevance quantification in explainable AI: A causal problem relevance quantification in explainable AI: A causal problem](http://proceedings.mlr.press/v108/janzing20a/janzing20a.pdf), AISTATS 2020

[You Shouldn’t Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods](https://umangsbhatt.github.io/reports/ecai.pdf), ECAI 2020

[Bias also matters: Bias attribution for deep neural network explanation](http://proceedings.mlr.press/v97/wang19p.html), ICML 2019

[Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation](https://arxiv.org/pdf/1902.00407.pdf), ICML 2019

[On the Connection Between Adversarial Robustness and Saliency Map Interpretability](https://arxiv.org/pdf/1905.04172.pdf), ICML 2019

[Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation](http://proceedings.mlr.press/v97/ancona19a/ancona19a.pdf), ICML 2019

[Explainability Techniques for Graph Convolutional Networks](https://arxiv.org/abs/1905.13686), ICML Workshop 2019

FullGrad, [Full-Gradient Representation for Neural Network Visualization](https://papers.nips.cc/paper/8666-full-gradient-representation-for-neural-network-visualization.pdf), NeurIPS 2019

[Towards Automatic Concept-based Explanations](https://papers.nips.cc/paper/2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf), NeurIPS 2019

[GNNExplainer: Generating Explanations for Graph Neural Networks](https://proceedings.neurips.cc/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html), NeurIPS 2019

[On the (In)fidelity and Sensitivity for Explanations](https://arxiv.org/abs/1901.09392v4), NeurIPS 2019

[Robust Attribution Regularization](https://papers.nips.cc/paper_files/paper/2019/file/172ef5a94b4dd0aa120c6878fc29f70c-Paper.pdf), NeurIPS 2019

[Explanations can be manipulated and geometry is to blame](https://proceedings.neurips.cc/paper/2019/file/bb836c01cdc9120a9c984c525e4b1a4a-Paper.pdf), NeurIPS 2019

[Interpretation of Neural Networks is Fragile](https://arxiv.org/abs/1710.10547), AAAI 2019

[XRAI: Better Attributions Through Regions](http://openaccess.thecvf.com/content_ICCV_2019/papers/Kapishnikov_XRAI_Better_Attributions_Through_Regions_ICCV_2019_paper.pdf), ICCV 2019

[Understanding Deep Networks via Extremal Perturbations and Smooth Masks](https://arxiv.org/pdf/1910.08485.pdf), ICCV 2019

[L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data](https://arxiv.org/pdf/1808.02610.pdf), ICLR 2019

[Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wagner_Interpretable_and_Fine-Grained_Visual_Explanations_for_Convolutional_Neural_Networks_CVPR_2019_paper.pdf), CVPR 2019

[Explainability Methods for Graph Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/Pope_Explainability_Methods_for_Graph_Convolutional_Neural_Networks_CVPR_2019_paper.pdf), CVPR 2019

[This Looks Like That: Deep Learning for Interpretable Image Recognition](http://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition.pdf), NeurIPS 2019

[“Why Should You Trust My Explanation?” Understanding Uncertainty in LIME Explanations](https://arxiv.org/pdf/1904.12991.pdf), ICML 2019

[Gradient-Based Vs. Propagation-Based Explanations: An Axiomatic Comparison](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_13), In book: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp.253-265, Springer 2019

[The Many Shapley Values for Model Explanation](https://arxiv.org/pdf/1908.08474.pdf), arxiv preprint 2019

[Explaining the Explainer: A First Theoretical Analysis of LIME](http://arxiv.org/abs/2001.03447), arxiv preprint 2020

VarGard,[Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values](https://arxiv.org/abs/1810.03307), ICLR 2018 workshop


NoiseTunnel, [Sanity checks for saliency maps](http://arxiv.org/abs/1810.03292), NeurIPS 2018

[Towards Robust Interpretability with Self-Explaining Neural Networks](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks), NeurIPS 2018

[Model Agnostic Supervised Local Explanations](https://arxiv.org/abs/1807.02910), NeurIPS 2018

Integrated Gradients, [Did the Model Understand the Question?](http://arxiv.org/abs/1805.05492), ACL 2018

Neuron Integrated Gradients: [Computationally Efficient Measures of Internal Neuron Importance](http://arxiv.org/abs/1807.09946) , preprint 2018

TCAV: [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279), ICML 2018

[A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations](http://proceedings.mlr.press/v80/nie18a/nie18a.pdf), ICML 2018

L2X: [Learning to Explain: An Information-Theoretic Perspective on Model Interpretation](https://arxiv.org/pdf/1802.07814.pdf), ICML 2018 [code](https://github.com/Jianbo-Lab/L2X)

[Noise-adding Methods of Saliency Map as Series of Higher Order Partial Derivative](https://arxiv.org/pdf/1806.03000.pdf), ICML 2018 workshop

InternalInfluence, [Influence-Directed Explanations for Deep Convolutional Networks](http://arxiv.org/abs/1802.03788), IEEE International Test Conference 2018

[Interpretable Basis Decomposition for Visual Explanation](https://openaccess.thecvf.com/content_ECCV_2018/papers/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.pdf), 2018 ECCV

[Grounding Visual Explanations](https://arxiv.org/abs/1807.09685), ECCV 2018

RuleMatrix: [RuleMatrix: Visualizing and Understanding Classifiers with Rules](https://arxiv.org/pdf/1807.06228.pdf), TVCG 2018

Manifold: [Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models](https://arxiv.org/pdf/1808.00196.pdf), TVCG 2018

[Top-down neural attention by excitation backprop](https://link.springer.com/article/10.1007/s11263-017-1059-x), IJCV 2018, (ECCV 2016)

[RISE: Randomized Input Sampling for Explanation of Black-box Models](http://bmvc2018.org/contents/papers/1064.pdf), BMVC 2018

Shap: [A unified approach to interpreting model predictions](http://arxiv.org/abs/1705.07874), NeurIPS 2017

[Real Time Image Saliency for Black Box Classifiers](https://papers.nips.cc/paper/7272-real-time-image-saliency-for-black-box-classifiers.pdf), NeurIPS 2017

[Explaining nonlinear classification decisions with deep Taylor decomposition](https://www.sciencedirect.com/science/article/pii/S0031320316303582), Pattern Recognition 2017

[Interpretable Explanations of Black Boxes by Meaningful Perturbation](https://arxiv.org/pdf/1704.03296.pdf), ICCV 2017

[Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-Supervised Object and Action Localization](https://ieeexplore.ieee.org/abstract/document/8237643), ICCV 2017

Grad-CAM: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization](http://arxiv.org/abs/1610.02391) ICCV 2017, IJCV 2019

[Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/pdf/1704.05796.pdf), CVPR 2017

DeepLIFT: [Learning important features through propagating activation differences](http://arxiv.org/abs/1704.02685), ICML 2017

Integrated Gradients: [Axiomatic attribution for deep networks](http://arxiv.org/abs/1703.01365), ICML 2017

SmoothGard: [SmoothGrad: removing noise by adding noise](http://arxiv.org/abs/1706.03825), ICML 2017

[Visualizing deep neural network decisions: Prediction difference analysis](https://arxiv.org/pdf/1702.04595.pdf), ICLR 2017

[Visualizing deep neural net- work decisions: Prediction difference analysis](https://arxiv.org/abs/1702.04595), arxiv preprint 2017

Lime: ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](http://arxiv.org/abs/1602.04938), SIGKDD 2016

[Visualizing deep convolutional neural networks using natural pre-images](https://arxiv.org/abs/1512.02017), IJCV 2016

[Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models](https://arxiv.org/pdf/1612.08468.pdf), Arxiv preprint 2016

[Salient deconvolutional networks](https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran16salient.pdf), ECCV 2016

[Top-down Neural Attention by Excitation Backprop](https://arxiv.org/pdf/1608.00507.pdf), ECCV 2016

LRP: [Layer-wise relevance propagation for neural networks with local renormalization layers](http://arxiv.org/abs/1604.00825), ICANN 2016

Gradient * input: [Not Just a Black Box: Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1605.01713), arxiv preprint 2016

[Investigating the influence of noise and distractors on the interpretation of neural networks](https://arxiv.org/pdf/1611.07270.pdf), NeurIPS 2016

QII, [Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems](https://ieeexplore.ieee.org/document/7546525), IEEE Symposium on Security and Privacy (SP)

epsilon-LRP, [On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4498753/), PloS one 2015

Perturbation-Based method, [Predicting effects of noncoding variants with deep learning–based sequence model](https://www.ncbi.nlm.nih.gov/pubmed/26301843), nature method 2015

CAM: [Learning Deep Features for Discriminative Localization](http://arxiv.org/abs/1512.04150), CVPR 2015

Guided Backpropagation, [Striving for simplicity: The all convolutional net](http://arxiv.org/abs/1412.6806), ICLR 2015

[Understanding neural networks through deep visualization](https://arxiv.org/abs/1506.06579), arxiv preprint 2015

Back progagation: [Deep inside convolutional networks: Visualising image classification models and saliency maps](http://arxiv.org/abs/1312.6034), ICLR 2014

Deconvnet: [Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901)

Shapley sampling values: [Explaining prediction models and individual predictions with feature contributions](https://dl.acm.org/doi/10.1007/s10115-013-0679-x), ACM Knowledge and Information Systems 2014

[Bounding the Estimation Error of Sampling-based Shapley Value Approximation](https://arxiv.org/pdf/1306.4265.pdf), arxiv preprint 2013

[Permutation importance: a corrected feature importance measure](https://academic.oup.com/bioinformatics/article/26/10/1340/193348), Bioinformatics 2010

[How to explain individual classification decisions](http://arxiv.org/abs/0912.1128), Journal of Machine Learning Research 2010

[An Efficient Explanation of Individual Classifications using Game Theory](https://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf), Journal of Machine Learning Research 2010

[Explaining Classifications for Individual Instances](http://lkm.fri.uni-lj.si/rmarko/papers/RobnikSikonjaKononenko08-TKDE.pdf), TKDE 2008

[Review and comparison of methods to study the contribution of variables in artificial neural network models](https://www.sciencedirect.com/science/article/abs/pii/S0304380002002570), Ecological Modelling 2003
